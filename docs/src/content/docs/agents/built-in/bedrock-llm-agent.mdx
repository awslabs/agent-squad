---
title: Bedrock LLM Agent
description: Documentation for the BedrockLLMAgent in the Multi-Agent Orchestrator
---
## Overview

The **Bedrock LLM Agent** is a powerful and flexible agent class in the Multi-Agent Orchestrator System. It leverages [Amazon Bedrock's Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) to interact with various LLMs supported by Amazon Bedrock.

This agent can handle a wide range of processing tasks, making it suitable for diverse applications such as conversational AI, question-answering systems, and more.

## Key Features

- Integration with Amazon Bedrock's Converse API
- Support for multiple LLM models available on Amazon Bedrock
- Streaming and non-streaming response options
- Customizable inference configuration
- Ability to set and update custom system prompts
- Optional integration with [retrieval systems](/multi-agent-orchestrator/retrievers/overview) for enhanced context
- Support for [Tool use](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html) within the conversation flow

## Creating a BedrockLLMAgent

By default, the **Bedrock LLM Agent** uses the `anthropic.claude-3-haiku-20240307-v1:0` model.

### Basic Example

To create a new **Bedrock LLM Agent** with only the required parameters, use the following code:

import { Tabs, TabItem } from '@astrojs/starlight/components';

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">
    ```typescript
    import { BedrockLLMAgent } from 'multi-agent-orchestrator';

    const agent = new BedrockLLMAgent({
      name: 'Tech Agent',
      description: 'Specializes in technology areas including software development, hardware, AI, cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs related to technology products and services.'
    });
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    ```python
    from multi_agent_orchestrator.agents import BedrockLLMAgent, BedrockLLMAgentOptions

    agent = BedrockLLMAgent(BedrockLLMAgentOptions(
        name='Tech Agent',
        description='Specializes in technology areas including software development, hardware, AI, cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs related to technology products and services.'
    ))
    ```
  </TabItem>
</Tabs>

In this basic example, only the `name` and `description` are provided, which are the only required parameters for creating a BedrockLLMAgent.

### Advanced Example

For more complex use cases, you can create a **Bedrock LLM Agent** with all available options. All parameters except `name` and `description` are optional:

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">
    ```typescript
    import { BedrockLLMAgent, BedrockLLMAgentOptions } from 'multi-agent-orchestrator';
    import { Retriever } from '../retrievers/retriever';

    const options: BedrockLLMAgentOptions = {
      name: 'My Advanced Bedrock Agent',
      description: 'A versatile agent for complex NLP tasks',
      modelId: 'anthropic.claude-3-sonnet-20240229-v1:0',
      region: 'us-west-2',
      streaming: true,
      inferenceConfig: {
        maxTokens: 1000,
        temperature: 0.7,
        topP: 0.9,
        stopSequences: ['Human:', 'AI:']
      },
      guardrailConfig: {
        guardrailIdentifier: 'my-guardrail',
        guardrailVersion: '1.0'
      },
      retriever: new Retriever(), // Assuming you have a Retriever class implemented
      toolConfig: {
        tool: [
          {
            type: 'function',
            function: {
              name: 'get_current_weather',
              description: 'Get the current weather in a given location',
              parameters: {
                type: 'object',
                properties: {
                  location: {
                    type: 'string',
                    description: 'The city and state, e.g. San Francisco, CA'
                  },
                  unit: { type: 'string', enum: ['celsius', 'fahrenheit'] }
                },
                required: ['location']
              }
            }
          }
        ]
      }
    };

    const agent = new BedrockLLMAgent(options);
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    ```python
    from multi_agent_orchestrator.agents import BedrockLLMAgent, BedrockLLMAgentOptions
    from multi_agent_orchestrator.retrievers import Retriever

    options = BedrockLLMAgentOptions(
        name='My Advanced Bedrock Agent',
        description='A versatile agent for complex NLP tasks',
        model_id='anthropic.claude-3-sonnet-20240229-v1:0',
        region='us-west-2',
        streaming=True,
        inference_config={
            'maxTokens': 1000,
            'temperature': 0.7,
            'topP': 0.9,
            'stopSequences': ['Human:', 'AI:']
        },
        guardrail_config={
            'guardrailIdentifier': 'my-guardrail',
            'guardrailVersion': '1.0'
        },
        retriever=Retriever(),  # Assuming you have a Retriever class implemented
        tool_config={
            'tool': [
                {
                    'toolSpec': {
                        'name': 'get_current_weather',
                        'description': 'Get the current weather in a given location',
                        'inputSchema': {
                            'json': {
                                'type': 'object',
                                'properties': {
                                    'location': {
                                        'type': 'string',
                                        'description': 'The city and state, e.g. San Francisco, CA'
                                    },
                                    'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}
                                },
                                'required': ['location']
                            }
                        }
                    }
                }
            ],
            'useToolHandler': lambda response, conversation: (False, response)  # Process tool response
        }
    )

    agent = BedrockLLMAgent(options)
    ```
  </TabItem>
</Tabs>

### Option Explanations

- `name` and `description`: Identify and describe the agent's purpose.
- `model_id`: Specifies the LLM model to use (e.g., Claude 3 Sonnet).
- `region`: AWS region for the Bedrock service.
- `streaming`: Enables streaming responses for real-time output.
- `inference_config`: Fine-tunes the model's output characteristics.
- `guardrail_config`: Applies predefined guardrails to the model's responses.
- `retriever`: Integrates a retrieval system for enhanced context.
- `tool_config`: Defines tools the agent can use and how to handle their responses.

## Setting a New Prompt


<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">
    ```typescript
    agent.setSystemPrompt(
      `You are an AI assistant specialized in {{DOMAIN}}.
       Your main goal is to {{GOAL}}.
       Always maintain a {{TONE}} tone in your responses.`,
      {
        DOMAIN: "cybersecurity",
        GOAL: "help users understand and mitigate potential security threats",
        TONE: "professional and reassuring"
      }
    );
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    ```python
    agent.set_system_prompt(
        """You are an AI assistant specialized in {{DOMAIN}}.
           Your main goal is to {{GOAL}}.
           Always maintain a {{TONE}} tone in your responses.""",
        {
            "DOMAIN": "cybersecurity",
            "GOAL": "help users understand and mitigate potential security threats",
            "TONE": "professional and reassuring"
        }
    )
    ```
  </TabItem>
</Tabs>

This method allows you to dynamically change the agent's behavior and focus without creating a new instance.

## Adding the Agent to the Orchestrator

To integrate the **Bedrock LLM Agent** into your orchestrator, follow these steps:

1. First, ensure you have created an instance of the orchestrator:

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">
    ```typescript
    import { MultiAgentOrchestrator } from "multi-agent-orchestrator";

    const orchestrator = new MultiAgentOrchestrator();
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    ```python
    from multi_agent_orchestrator.orchestrator import MultiAgentOrchestrator

    orchestrator = MultiAgentOrchestrator()
    ```
  </TabItem>
</Tabs>

2. Then, add the agent to the orchestrator:

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">
    ```typescript
    orchestrator.addAgent(agent);
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    ```python
    orchestrator.add_agent(agent)
    ```
  </TabItem>
</Tabs>

3. Now you can use the orchestrator to route requests to the appropriate agent, including your Bedrock LLM agent:

<Tabs syncKey="runtime">
  <TabItem label="TypeScript" icon="seti:typescript" color="blue">
    ```typescript
    const response = await orchestrator.routeRequest(
      "What is the base rate interest for 30 years?",
      "user123",
      "session456"
    );
    ```
  </TabItem>
  <TabItem label="Python" icon="seti:python">
    ```python
    response = await orchestrator.route_request(
        "What is the base rate interest for 30 years?",
        "user123",
        "session456"
    )
    ```
  </TabItem>
</Tabs>

---

By leveraging the **Bedrock LLM Agent**, you can create sophisticated, context-aware AI agents capable of handling a wide range of tasks and interactions, all powered by the latest LLM models available through Amazon Bedrock.